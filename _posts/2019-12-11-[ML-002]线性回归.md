---
layout: post
title:  "[ML-002]线性回归"
date: 2019-12-11 11:08:07 +0800
categories: [机器学习]
---

# 回归(Regression)和分类(Classification)
在[[ML-001]机器学习简介](https://joestarhu.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2019/12/09/ML-001-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/)
中讲到了监督学习的方向有回归和分类
- 回归(Regression):输出值是连续的,那么就是回归(比如,气温是多少度,这是一个连续的值)
- 分类(Classification):输出值是离散的,那么就是分类(比如,天气是晴天还是下雨,这是一个离散的值)

# 线性回归(Linear Regression)
线性回归是**监督学习**的**回归算法**
它的模型为:
> Y = W0+W1\*X1+W2\*X2 +···+ Wn\*Xn

|模型参数|参数说明|
|:--|:--|
|Y|输出值|
|W0|截距,intercept,可设置或不设置|
|W1~Wn|权重系数(Coefficient Weight)|
|X1~Xn|输入值|

# 矩阵(Matrix)
先看一个简单的数学问题
```
假设y=b+w*x,已知x和y如下:
x:1,y:2.5
x:2,y:4.5
x:3,y:6.5
x:4,y:8.5
x:5,y:10.5
求解x为6-10的时候,y的值
```
上述问题就是一个简单的线性回归问题  
- x,y可以理解为是训练数据集,其中y是标签数据  
- w就是权重,b就是截距(intercept)
- 通过现有的x和y的值,去求解w和b的过程,就称为学习的过程
- 求解x为6到10的值,这个在机器学习里面我们称为**泛化能力(genralzation)** ,意味着求解新样本的能力

从公式中可以看出y=b\*1+w\*x,那么b和w以及1和x可以组成2个矩阵,变成矩阵的乘法.  
![矩阵乘法](https://github.com/joestarhu/joestarhu.github.io/blob/master/pic/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95.png?raw=true)

从图上我们可以看到,x变为(5,2)的矩阵,w变成(2,1)的矩阵,最后得到的y为(5,1)的矩阵

因此,我们的线性回归可以定义如下
``` python
class LinearRegression:
  def __init__(self,intercept=True):
    self.__inter = intercept
    self.W = None

  def __input_init(self,X):
    if self.__inter:
      # 组合ndarray数组
      X = np.c_[np.ones(X.shape[0]),X]
    return X

  def predict(self,X):
    X = self.__input_init(X)
    # numpy的矩阵乘法可以用@符号,也可用方法np.dot(X,W)或X.dot(W)
    return X @ self.W
```

那么如何去求解w和b的值呢? 有2种方法
- 正规方程求解
- 梯度下降求解

# 正规方程(Normal Equation)
这是一个十分快速而且便捷的求解方法,它的公式为:
![正规方程](https://github.com/joestarhu/joestarhu.github.io/blob/master/pic/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png?raw=true)

``` python
class LinearRegression:
  def fit_normal_equation(self,X,Y):
    X = self.__input_init(X)
    # 这里用pinv伪逆来替代inv逆函数,是防止出现矩阵不可逆的现象
    self.W = np.linalg.pinv(X.T @ X) @ X.T @ Y
```

OK现在我们来尝试下求解w和b(给出完整代码)
``` python
import numpy as np

class LinearRegression:
  def __init__(self,intercept=True):
    self.__inter = True
    self.W = None

  def __input_init(self,X):
    if self.__inter:
      X = np.c_[np.ones(X.shape[0]),X]
    return X

  def fit_normal_equation(self,X,Y):
    X = self.__input_init(X)
    self.W = np.linalg.pinv(X.T @ X) @ X.T @ Y

  def predict(self,X):
    X = self.__input_init(X)
    return X @ self.W

if __name__ == '__main__':
  x = np.array([1,2,3,4,5],dtype=np.float).reshape(-1,1)
  y = np.array([2.5,4.5,6.5,8.5,10.5]).reshape(-1,1)
  lre = LinearRegression()
  lre.fit_normal_equation(x,y)
  print(lre.W) #  可以得到结果为0.5,2
  # 然后求解6-10
  t = np.arange(6,11).reshape(-1,1)
  lre.predict(t)
```
OK,到这里我们基本上已经完成了一个简单的线性模型.通过正规方程可以快速的解出权重参数,但是正规方程也有缺点,就是当输入样本数量很大的时候,计算会变得缓慢,一般来说如果输入样本没有超过1万的时候,使用正规方程来求解,如果超过则使用梯度下降来求解

# 代价函数(Cost Function)
到目前为止,从ML的定义中,我们已经接触了E和T
- E:就是输入值
- T:就是预测输出

那么用来衡量性能P的是什么?这里就引入代价函数,他来衡量我们的机器学习表现.代价函数的值越小,代表越好

线性回归的代价函数用Mean Squared Error(均方差函数),简写为MSE.
``` python
class LinearRegression:
  def mse(self,X,Y):
    size = X.shape[0]
    mse = 0.5 * np.sum((self.predict(X)-Y)**2) / size
    return mse
```

# 导数(Derivative)和偏导数(Partial Derivative)和梯度(Gradient)
导数的几何意义就是某点上切线斜率,当这个的斜率为0的时候,代表这是它的极小值(如果是凸函数,那么就一定是最小值)

![导数的意义](https://github.com/joestarhu/joestarhu.github.io/blob/master/pic/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%9C%80%E5%B0%8F%E5%80%BC%E8%A7%A3%E6%9E%90.png?raw=true)

导数的求解用数值微分:`d(x) = (f(x+h)-f(x-h)) / (2*h)`
- d(x):对x求导的导数值
- f(x):求导函数
- h:一个微小的数值

``` python
def fn(x):
  return 2*x+1

def deri(x,fn):
  h = 1e-5
  f1 = fn(x+h)
  f2 = fn(x-h)
  return (f1-f2)/(2*h)

# deri求解出来的值都是约等于2.
deri(1.,fn)
deri(3.,fn)
deri(5.,fn)
```

偏导数:相当于对多个变量求导数,求出来的值,我们可以称为是梯度(gradient)
``` python
def fn(X):
  return X[0]**2 + X[1]**2

def grad(X,fn):
  h = 1e-5
  grad = np.zeros_like(X)
  for i in range(X.shape[0]):
    t = X[i]
    X[i] = t + h
    f1 = fn(X)
    X[i] = t - h
    f2 = fn(X)
    grad[i] = (f1 - f2)/(2*h)
    X[i] = t
  return grad

# 这里注意下哦,如果不改类型为float,会出现比较大的数值偏差
x = np.array([2,4],dtype=np.float)
fn(x)
grad(x,fn)
```

# 梯度下降(Gradient Descent)
正如我们前面所说,衡量线性回归的P是MSE,MSE的值越小,代表表现的越好.  
如何让MSE的值变小呢?那么就要沿着梯度的方向下降即可,即导数为零的地方.
梯度下降法需要:
- 1. 设定学习率LearningRate和迭代次数Iteration
- 2. 梯度的更新要同步进行

``` python
class LinearRegression:
  def gradient descent(self,X,Y,W,lr,iter):
    h = 1e-5
    self.W = W
    grad = np.zeros_like(self.W)
    for _ in range(iter):
      for i in range(grad.shape[0]):
        t = self.W[i]
        self.W[i] = t + h
        f1 = self.mse(X,Y)
        self.W[i] = t - h 
        f2 = self.mse(X,Y)
        grad[i] = (f1-f2) / (2*h)
        self.W[i] = t
      self.W -= lr*grad
```

- 如果设置的LearningRate过于小,那么会造成收敛速度太慢
- 如果设置的LearningRate太大,那么会造成无法收敛




